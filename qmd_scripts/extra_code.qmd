---
title: "Winner Prediction"
format: 
  html:
    toc: true
    toc-depth: 3
---

# Loading Data

```{python}
import pandas as pd
import numpy as np
import plotly.express as px
import plotnine as pn
import seaborn as sns
import matplotlib.pyplot as plt
from janitor import clean_names
from matplotlib import rcParams
import pyarrow as pa
import pyarrow.parquet as pq
import fastf1
import duckdb as duck

pd.set_option('mode.copy_on_write', True)
pd.set_option('display.max_columns', None)
rcParams.update({'savefig.bbox': 'tight'})
```

# Circuit Map

```{python}
#| eval: false
#| echo: false

def rotate(xy, *, angle):
    """Rotate a DataFrame or Series of x/y points."""
    rot_mat = np.array([[np.cos(angle), np.sin(angle)],
                        [-np.sin(angle), np.cos(angle)]])
    # xy can be a DataFrame (n, 2) or Series/list (2,)
    if isinstance(xy, pd.DataFrame):
        return xy @ rot_mat
    else:  # assume it's Series or list-like with 2 elements
        return pd.Series(xy) @ rot_mat
```

```{python}
driver_number22 = ['1', '3', '4', '5', '6', '10', '11', '14', '16', '18', '20', '22', '23', '24', '31', '44', '47', '55', '63', '77']

driver_number23 = ['1', '2', '4', '10', '11', '14', '16', '18', '20', '21', '22', '23', '24', '27', '31', '44', '55', '63', '77', '81']

driver_number24 = ['1', '2', '3', '4', '10', '11', '14', '16', '18', '20', '22', '23', '24', '27', '44', '55', '61', '63', '77', '81']
driver_number24_2 = ['1', '2', '3', '4', '10', '11', '14', '16', '18', '20', '22', '23', '24', '27', '31', '44', '55', '63', '77', '81']

driver_number22 = [int(x) for x in driver_number22]
driver_number23 = [int(x) for x in driver_number23]
driver_number24 = [int(x) for x in driver_number24]
driver_number24_2 = [int(x) for x in driver_number24_2]
```

# Laps Data

```{python}
laps = pq.read_table('canada_laps.parquet')
laps = laps.to_pandas()

laps['lap_time_sec'] = laps['LapTime'].dt.total_seconds()
laps['sector1_sec'] = laps['Sector1Time'].dt.total_seconds()
laps['sector2_sec'] = laps['Sector2Time'].dt.total_seconds()
laps['sector3_sec'] = laps['Sector3Time'].dt.total_seconds()
laps['sector1_sessiontime_sec'] = laps['Sector1SessionTime'].dt.total_seconds()
laps['sector2_sessiontime_sec'] = laps['Sector2SessionTime'].dt.total_seconds()
laps['sector3_sessiontime_sec'] = laps['Sector3SessionTime'].dt.total_seconds()
laps['pit_out_time_sec'] = laps['PitOutTime'].dt.total_seconds()
laps['pit_in_time_sec'] = laps['PitInTime'].dt.total_seconds()
laps['time_sec'] = laps['Time'].dt.total_seconds()

laps = laps.sort_values(['Driver', 'LapNumber', 'year'], ascending = True)

laps['has_pit_out'] = np.where(laps['pit_out_time_sec'].isna(), 0, 1)
laps['has_pit_in'] = np.where(laps['pit_in_time_sec'].isna(), 0, 1)

laps[['pit_out_time_sec', 'pit_in_time_sec']] = laps[['pit_out_time_sec', 'pit_in_time_sec']].fillna(0)
laps['pit_in_time_sec_shift'] = laps['pit_in_time_sec'].shift(1)
laps['pit_in_time_sec_shift'] = laps['pit_in_time_sec_shift'].fillna(0)

laps['pit_delta'] = laps['pit_out_time_sec'] - laps['pit_in_time_sec_shift']

laps['time_join'] = laps['time_sec'].round(0)
```

# Weather Data

```{python}
weather = pq.read_table('canada_weather.parquet')
weather = weather.to_pandas()

weather = weather.drop(columns = 'time_sec')
weather['time_sec'] = weather['Time'].dt.total_seconds()

weather['time_start'] = weather['time_sec'].round(0)
weather['time_end'] = weather['time_start'].shift(-1).fillna(0)
weather['interval'] = weather.groupby(['session', 'year']).cumcount() + 1
```

# Car Data

```{python}
cardata = pq.read_table('canada_cardata.parquet')
cardata = cardata.to_pandas()

cardata = cardata.drop(columns = ['date_sec', 'time_sec', 'session_time_sec'])

cardata['time_sec'] = cardata['Time'].dt.total_seconds()
cardata['session_time_sec'] = cardata['SessionTime'].dt.total_seconds()
cardata['date'] = cardata['Date'].dt.date
cardata['time'] = cardata['Date'].dt.time

cardata = cardata.drop(columns = ['Date', 'Time', 'SessionTime'])
```

# POS Data

```{python}
pos = pq.read_table('canada_pos.parquet')
pos = pos.to_pandas()

pos = pos.drop(columns = ['date_sec', 'time_sec', 'session_time_sec'])

pos['date'] = pos['Date'].dt.date
pos['time'] = pos['Date'].dt.time
pos['time_sec'] = pos['Time'].dt.total_seconds()
pos['session_time_sec'] = pos['SessionTime'].dt.total_seconds()

pos = pos.drop(columns = ['Date', 'Time', 'SessionTime'])
```

```{python}
#| eval: false
#| echo: false

# gp_24 = fastf1.get_session(2024, track_loc, 'R')
# gp_24.load()
# track_layout = gp_24.get_circuit_info()

# need fixing below
track_pos = pos.loc[(pos['driver_number'] == '1'), ('X', 'Y')]

track_angle = can_track.rotation / 180 * np.pi

rotated_track = rotate(track_pos, angle = track_angle)

rotated_track = rotated_track.loc[(rotated_track[0] != 0) & (rotated_track[1] != 0)]
rotated_track['X'] = rotated_track[0]
rotated_track['Y'] = rotated_track[1]
rotated_track = rotated_track.drop(columns = [0, 1])

offset_vector = pd.Series([500, 0]) 

pn.ggplot.show(
  pn.ggplot(rotated_track, pn.aes('X', 'Y'))
  + pn.geom_path()
) 
```

# Connecting to Database

```{python}
import duckdb as duck
from sqlalchemy import create_engine

engine = create_engine('duckdb:///f1_gp.duckdb') #connect_args={'read_only': False, 'config': {'memory_limit': '500mb'}})

# engine = create_engine('sqlite://', echo = False)
```

```{python}
match_data.to_sql('lap_weather', engine, if_exists = 'replace', index = False)
cardata.to_sql('cardata', engine, if_exists = 'replace', index = False)
pos.to_sql('pos', engine, if_exists = 'replace', index = False)
```

```{python}
#| eval: false
#| echo: false

from great_tables import GT

# Calculations & Table Creation
fastest_lap = laps.groupby(['year', 'Driver', 'session'])['LapTime'].min().reset_index().sort_values(['session'])

# laps.groupby(['year', 'Driver', 'session'])['LapNumber'].max().reset_index().sort_values('session')

laps.loc[laps['LapTime'].isin(fastest_lap['LapTime']),
['Driver', 'year', 'session', 'LapTime', 'LapNumber', 'Stint', 'Sector1Time', 
'Sector2Time', 'Sector3Time', 'Compound', 'TyreLife', 'FreshTyre']].sort_values(
  'session', 
  ascending = True).drop_duplicates(
    ['LapTime', 'Driver','session', 'year'])
```

```{python}
laps.columns.tolist()

weather.columns.tolist()

# need to include pitlane delta

laps = laps.sort_values(['session', 'Driver', 'year'])

laps['has_pit_out'] = np.where(laps['pit_out_time_sec'].isna(), 0, 1)
laps['has_pit_in'] = np.where(laps['pit_in_time_sec'].isna(), 0, 1)

laps.head()

pn.ggplot.show(
  pn.ggplot(laps.melt(id_vars = ['Driver', 'session', 'year'],
  value_vars = ['has_pit_out', 'has_pit_in']),
  pn.aes('value'))
  + pn.geom_bar(pn.aes(fill = 'factor(variable)'), color = 'black', position = 'dodge')
  + pn.facet_grid('session', 'Driver', scales = 'free')
  + pn.theme_classic()
)

laps[['pit_out_time_sec', 'pit_in_time_sec']] = laps[['pit_out_time_sec', 'pit_in_time_sec']].fillna(0)
laps['pit_in_time_sec_shift'] = laps['pit_in_time_sec'].shift(1)
laps['pit_in_time_sec_shift'] = laps['pit_in_time_sec_shift'].fillna(0)

laps.head()

laps['pit_delta'] = laps['pit_out_time_sec'] - laps['pit_in_time_sec_shift']

laps[['session', 'year', 'Driver', 'LapNumber', 'pit_out_time_sec', 'pit_in_time_sec', 'pit_in_time_sec_shift', 'pit_delta']].sort_values(['session', 'year', 'Driver', 'LapNumber'])

laps.columns.tolist()

laps.head()
weather.head()

laps['time_join'] = laps['time_sec'].round(0)
```

```{python}
weather['time_start'] = weather['time_sec'].round(0)
weather['time_end'] = weather['time_start'].shift(-1).fillna(0)

weather['interval'] = weather.groupby(['session', 'year']).cumcount() + 1

weather
```


```{python}
def match_intervals_grouped(df1, df2):
    matched_rows = []

    # Group both dataframes by year and session
    for (year, session), group1 in df1.groupby(['year', 'session']):
        group2 = df2[(df2['year'] == year) & (df2['session'] == session)]

        # For each row in df1 group, find matching interval in df2
        for _, row in group1.iterrows():
            value = row['time_join']
            matched = group2[(group2['time_start'] <= value) & (value < group2['time_end'])]
            if not matched.empty:
                interval_id = matched.iloc[0]['interval']
            else:
                interval_id = None

            row_out = row.copy()
            row_out['interval'] = interval_id
            matched_rows.append(row_out)

    return pd.DataFrame(matched_rows)

# Apply matching
match_data = match_intervals_grouped(laps, weather)

match_data = match_data.merge(weather, 'left', ['session', 'year', 'interval'])

match_data['lap_start_time_sec'] = match_data['LapStartTime'].dt.total_seconds()
match_data['lap_start_date'] = match_data['LapStartDate'].dt.date
match_data['lap_start_dt_time'] = match_data['LapStartDate'].dt.time

match_data = match_data.drop(columns = ['Time_x', 'LapTime', 'PitOutTime', 'PitInTime', 'Sector1Time', 'Sector2Time', 'Sector3Time', 'Sector1SessionTime', 'Sector2SessionTime', 'Sector3SessionTime', 'IsPersonalBest', 'LapStartTime', 'LapStartDate', 'Time_y', 'DeletedReason', 'FastF1Generated', 'IsAccurate'])
```

```{python}
model_laps = match_data.loc[:, ['Driver', 'DriverNumber', 'LapNumber', 'session', 'year', 'interval',
'Stint', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST',
'Compound', 'TyreLife', 'FreshTyre', 'time_sec_x',
'lap_time_sec', 'sector1_sec', 'sector2_sec', 'sector3_sec',
'sector1_sessiontime_sec', 'sector2_sessiontime_sec', 'sector3_sessiontime_sec',
'pit_out_time_sec', 'pit_in_time_sec', 'has_pit_out', 'has_pit_in',
'pit_in_time_sec_shift', 'pit_delta', 'time_join',
'interval', 'AirTemp', 'Humidity', 'Pressure',
'Rainfall', 'TrackTemp', 'WindDirection',
'WindSpeed',
'Position']]

model_laps.head()
```

## Lap Visualizations

```{python}
#| eval: false
#| echo: false

model_laps.groupby(['year', 'Driver', 'Compound', 'FreshTyre', 'session'])[['lap_time_sec', 'sector1_sec', 'sector2_sec', 'sector3_sec']].min().reset_index()

pn.ggplot.show(
  pn.ggplot(
    model_laps.groupby(
      ['year', 'LapNumber','Driver', 'Compound', 'FreshTyre', 'session'])[['lap_time_sec', 'sector1_sec', 'sector2_sec', 'sector3_sec']].min().reset_index().melt(
        id_vars = ['year', 'LapNumber', 'Driver', 'Compound', 'FreshTyre', 'session'], 
        value_vars = ['lap_time_sec', 'sector1_sec', 'sector2_sec', 'sector3_sec']),
        pn.aes('LapNumber', 'value'))
  + pn.geom_line(pn.aes(color = 'Driver', group = 'Driver'))
  + pn.geom_point(pn.aes(shape = 'Compound'), alpha = .7)
  + pn.facet_grid('variable', 'session', scales = 'free')
  + pn.theme_classic()
)

model_laps.groupby(['year', 'Driver', 'Compound', 'FreshTyre', 'session'])[['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']].max().reset_index()

#pn.ggplot.show(
#  pn.ggplot(laps_sub.groupby(['year', 'LapNumber','Driver', 'Compound', 'FreshTyre', 'session'])[['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']].max().reset_index().melt(id_vars = ['year', 'LapNumber', 'Driver', 'Compound', 'FreshTyre', 'session'], value_vars = ['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']),
#  pn.aes('LapNumber', 'value'))
#  + pn.geom_line(pn.aes(color = 'Driver', group = 'Driver'))
#  + pn.geom_point(pn.aes(shape = 'Compound'), alpha = .7)
#  + pn.scale_shape_manual(values = ['v', '*', 'o', '+'])
#  + pn.facet_grid('variable', 'session', scales = 'free')
#  + pn.theme_classic()
#)
```

```{python}
#| eval: false
#| echo: false

model_laps.head()

# impute with mean value
model_laps.loc[(model_laps.isna().any(axis = 1))]

model_laps.loc[(model_laps['SpeedI1'].isna())]

model_laps.loc[(model_laps['SpeedI1'].isna()), 'SpeedI1'].isna().sum()
```

```{python}
mean_imp = model_laps.groupby(
    ['Driver', 'session', 'year'])[['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'sector1_sec', 'sector2_sec', 'sector3_sec']].transform(
      lambda x: x.fillna(x.mean()))

model_laps = model_laps.drop(columns = ['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'sector1_sec', 'sector2_sec', 'sector3_sec']).join(mean_imp)

model_laps.info()

# model_laps['sector2_sessiontime_sec'] = model_laps['sector2_sessiontime_sec'].fillna(model_laps['sector1_sessiontime_sec'] + model_laps['sector2_sec'])
# model_laps['sector3_sessiontime_sec'] = model_laps['sector3_sessiontime_sec'].fillna(model_laps['sector2_sessiontime_sec'] + model_laps['sector3_sec'])
model_laps['lap_time_sec'] = model_laps['lap_time_sec'].fillna(model_laps['sector1_sec'] + model_laps['sector2_sec'] + model_laps['sector3_sec'])

model_laps.columns.tolist()
```

```{python}
#| eval: false
#| echo: false

model_var_corr = model_laps.loc[:, ['sector1_sessiontime_sec', 'sector2_sessiontime_sec', 'sector3_sessiontime_sec', 'lap_time_sec']].corr()

corr_bot_only = np.triu(np.ones_like(model_var_corr, dtype = bool))

plt.clf()

sns.heatmap(model_var_corr, mask = corr_bot_only, annot = True)
plt.show()
```

```{python}
# def reserve_driver(df):
#   if df['year'] == 2022:
#     return 1 if df['DriverNumber'] in driver_number22 else 0
#   elif df['year'] == 2023:
#     return 1 if df['DriverNumber'] in driver_number23 else 0
#   else:
#     return 1 if df['DriverNumber'] in driver_number24 else 0
# 
# model_laps['reserve_driver'] = model_laps.apply(reserve_driver, axis = 1)

# model_laps.loc[(model_laps['year'] == 2024) & (model_laps['DriverNumber'] == 31), ['DriverNumber', 'reserve_driver']]

model_laps = model_laps.drop(columns = ['time_sec_x', 'sector1_sessiontime_sec', 'sector2_sessiontime_sec', 'sector3_sessiontime_sec', 'pit_in_time_sec'])

model_laps.loc[(model_laps.isna().any(axis = 1))]

model_laps['Position'] = model_laps['Position'].fillna(20)
```

# Modeling

```{python}
model_laps.loc[(model_laps.isna().any(axis = 1))]

mean_imp_gen = model_laps.groupby(
    ['session', 'year'])[['lap_time_sec', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'sector1_sec', 'sector2_sec', 'sector3_sec']].transform(
      lambda x: x.fillna(x.mean()))

model_laps = model_laps.drop(columns = ['lap_time_sec', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'sector1_sec', 'sector2_sec', 'sector3_sec']).join(mean_imp_gen)

model_laps.info()
model_laps.head()

model_laps[pd.get_dummies(model_laps['Driver'], dtype = 'float64').columns] = pd.get_dummies(model_laps['Driver'], dtype = 'float64')
model_laps[pd.get_dummies(model_laps['DriverNumber'], dtype = 'float64').columns] = pd.get_dummies(model_laps['DriverNumber'], dtype = 'float64')
model_laps[['fp1', 'fp2', 'fp3', 'q', 'r']] = pd.get_dummies(model_laps['session'], dtype = 'float64')
model_laps[['hard', 'intermediate', 'medium', 'soft', 'wet']] = pd.get_dummies(model_laps['Compound'], dtype = 'float64')
model_laps['fresh_tires'] = pd.get_dummies(model_laps['FreshTyre'], dtype = 'float64').iloc[:, 1]
model_laps['rain'] = pd.get_dummies(model_laps['Rainfall'], dtype = 'float64').iloc[:, 1]

model_laps.columns.tolist()
model_laps.info()

np.corrcoef(model_laps['sector1_sec'], model_laps['Position'])
np.corrcoef(model_laps['sector2_sec'], model_laps['Position'])
np.corrcoef(model_laps['sector3_sec'], model_laps['Position'])
np.corrcoef(model_laps['lap_time_sec'], model_laps['Position'])


model_laps = model_laps.drop(
  columns = ['Driver', 'DriverNumber', 'session', 'Compound', 'FreshTyre', 'Rainfall'])

model_laps.info()
model_laps.head()

# Should be Cleaning Data For FP laps that drop off to save tires and fuel
# model_laps = model_laps.loc[(fp1_laps_sub['LapNumber'] != 26)]

model_laps.shape
model_laps.dropna().shape

model_laps = model_laps.dropna()

model_laps['position_bi'] = np.where(model_laps['Position'] == 1, 1, 0)

model_laps_train = model_laps.loc[((model_laps['year'].isin([2022, 2023])) & (model_laps['r'] == 1)) | ((model_laps['year'] != 2024) & (model_laps['r'] != 1))]
model_laps_test = model_laps.loc[((model_laps['year'] == 2024) & (model_laps['r'] == 1))]

model_laps_train.shape
model_laps_test.shape

model_laps['position_bi'].value_counts(normalize = True)

# y_train = pd.Categorical(y_train, ordered = True)
# y_test = pd.Categorical(y_test, ordered = True)

```

```{python}
x_train = model_laps_train.drop(columns = ['Position', 'position_bi']).loc[(model_laps_train['year'] == 2022)]
x_val = model_laps_train.drop(columns = ['Position', 'position_bi']).loc[(model_laps_train['year'] == 2023)]
x_test = model_laps_test.drop(columns = ['Position', 'position_bi'])

# y_train = model_laps_train.loc[(model_laps_train['year'] == 2022), 'Position']
# y_val = model_laps_train.loc[(model_laps_train['year'] == 2023), 'Position']
# y_test = model_laps_test['Position']

y_train_bi = model_laps_train.loc[(model_laps_train['year'] == 2022), 'position_bi']
y_val_bi =  model_laps_train.loc[(model_laps_train['year'] == 2023), 'position_bi']
y_test_bi = model_laps_test['position_bi']

from imblearn.over_sampling import SMOTE

sm = SMOTE(sampling_strategy = .7, random_state = 14042025)
x_train_smote, y_train_smote = sm.fit_resample(x_train, y_train_bi)
x_val_smote, y_val_smote = sm.fit_resample(x_val, y_val_bi)
x_test_smote, y_test_smote = sm.fit_resample(x_test, y_test_bi)

from collections import Counter
Counter(y_train_bi)
Counter(y_train_smote)

```

# Keras Model

```{python}
#| eval: false
#| echo: false

from tensorflow.random import set_seed
from sklearn.metrics import accuracy_score, roc_auc_score, mean_absolute_error, roc_curve, make_scorer, log_loss, confusion_matrix
import tensorflow as tf
from tensorflow import keras
import keras_tuner as kt
#from keras.utils import FeatureSpace
from sklearn.utils.class_weight import compute_class_weight
import tensorflow.keras.models as keras_model
from tensorflow.keras.regularizers import l1_l2
from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, BatchNormalization, LayerNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.metrics import SparseCategoricalAccuracy
from tensorflow.keras.losses import SparseCategoricalCrossentropy

num_classes = np.unique(y_train)

class_wt = compute_class_weight('balanced', classes = num_classes, y = y_train)
class_wt = dict(enumerate(class_wt))

y_train_nn = np.array(y_train)
y_test_nn = np.array(y_test)

y_train_nn = y_train_nn - 1
y_test_nn = y_test_nn - 1

x_train_nn = np.array(x_train)
x_test_nn = np.array(x_test)

np.unique(y_train_nn)

num_classes
```

```{python}
#| eval: false
#| echo: false

set_seed(14042025)
nn = keras_model.Sequential([
    Dense(64, activation = 'relu', input_shape = (x_train.shape[1],)),
    #BatchNormalization(),
    LayerNormalization(),
    Dropout(.3),
    Dense(32, activation = 'relu'),
    #BatchNormalization(),
    LayerNormalization(),
    Dropout(.3),
    Dense(16, activation = 'relu'),
    #BatchNormalization(),
    LayerNormalization(),
    Dropout(.3),
    Dense(20, activation = 'softmax')
])

# kernel_initializer='he_normal'

# Compile model
nn.compile(
  optimizer = tf.keras.optimizers.AdamW(0.001),
  loss = [SparseCategoricalCrossentropy()],
  metrics = [SparseCategoricalAccuracy()])

nn.summary()
```

```{python}
#| eval: false
#| echo: false

# Train model
set_seed(14042025)
nn.fit(x_train_nn, y_train_nn, epochs = 100, batch_size = 32, class_weight = class_wt)
nn.evaluate(x_test_nn, y_test_nn)

# Sparse Categorical Cross Entropy = 1.55
# Sparse Categorical Accuracy = .566
```

```{python}
#| eval: false
#| echo: false

set_seed(14042025)
def build_model(hp):
    model = keras_model.Sequential()

    hp_unit1 = hp.Choice("units", values = [64, 128, 256, 512])
    hp_l1 = hp.Float('l1_pen', min_value = 0, max_value = 1, step = .1, default = 0)
    hp_l2 = hp.Float('l2_pen', min_value = 0, max_value = 1, step = .1, default = 0)
    model.add(Dense(hp_unit1, activation = 'relu', kernel_regularizer = l1_l2(l1 = hp_l1, l2 = hp_l2), input_shape = (x_train.shape[1],)))
    model.add(LayerNormalization())
    hp_dropout = hp.Float('dropout', min_value = .2, max_value = .7, step = .1, default = .2)
    model.add(Dropout(hp_dropout))

    hp_unit2 = hp.Choice("units2", values = [32, 64, 128, 256])
    model.add(Dense(hp_unit2, activation = 'relu', kernel_regularizer = l1_l2(l1 = hp_l1, l2 = hp_l2)))
    model.add(LayerNormalization())
    model.add(Dropout(hp_dropout))

    hp_unit3 = hp.Choice("units3", values = [2, 4, 8, 16, 32, 64, 128])
    model.add(Dense(hp_unit3, activation = 'relu', kernel_regularizer = l1_l2(l1 = hp_l1, l2 = hp_l2)))
    model.add(LayerNormalization())
    model.add(Dropout(hp_dropout))

    model.add(Dense(20, activation = 'softmax'))

    learn_rate = hp.Float("learning_rate", min_value = .0001, max_value = .01, step = .01)

    model.compile(
      optimizer = tf.keras.optimizers.AdamW(learning_rate = learn_rate),
      loss = [SparseCategoricalCrossentropy()],
      metrics = [SparseCategoricalAccuracy()])

    return model
```

```{python}
#| eval: false
#| echo: false

set_seed(14042025)
tuner = kt.RandomSearch(
  build_model,
  objective = 'sparse_categorical_accuracy',
  max_trials = 10,
  executions_per_trial = 1,
  project_name = 'tuned_gp'
)

set_seed(14042025)
tuner.search(x_train_nn,
             y_train_nn,
             #validation_split=0.2,
             epochs = 100,
             batch_size = 32,
             class_weight = class_wt,
             callbacks = [keras.callbacks.EarlyStopping(patience = 5)])
```

```{python}
#| eval: false
#| echo: false

best_model = tuner.get_best_models(num_models = 1)[0]
best_hps = tuner.get_best_hyperparameters(1)[0]

set_seed(14042025)
best_model.evaluate(x_test_nn, y_test_nn)
# Sparse Categorical Cross Entropy = 31.48
# Sparse Categorical Accuracy = .545

best_hps.values
```

```{python}
#| eval: false
#| echo: false

y_pred = nn.predict(x_test_nn)

# Convert predicted probabilities to class labels
y_pred_label = np.argmax(y_pred, axis=1)

# Get the confusion matrix
# cm = confusion_matrix(y_train_nn, y_pred_label)

# plt.clf()
# sns.heatmap(cm, annot = True, cbar = False)
# plt.show()
```

# Random Forest Classifier

```{python}
from tensorflow.random import set_seed
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, roc_auc_score, mean_absolute_error, roc_curve, make_scorer, log_loss, confusion_matrix, balanced_accuracy_score
from sklearn.ensemble import RandomForestClassifier as rfc, GradientBoostingClassifier as gbc
from imblearn.ensemble import BalancedRandomForestClassifier as brfc, RUSBoostClassifier as rus
from sklearn.utils.class_weight import compute_class_weight
from joblib import parallel_backend, Parallel, parallel_config

class_wt = compute_class_weight('balanced', classes = np.unique(y_train_smote), y = y_train_smote)
class_wt = dict(enumerate(class_wt))

# Random Forest 
set_seed(14042025)
rf_model = rfc(
  criterion = 'log_loss',
  n_estimators = 1000,
  min_samples_split = 2,
  max_depth = 10,
  random_state = 14042025, 
  verbose = 2,
  n_jobs = 10,
  class_weight = class_wt)

rf_model.fit(x_train, y_train_bi)
rf_pred = rf_model.predict(x_val)
confusion_matrix(y_val_bi, rf_pred)
accuracy_score(y_val_bi, rf_pred)
roc_auc_score(y_val_bi, rf_pred)

# rf_model.fit(x_train_smote, y_train_smote)
# rf_pred_smote = rf_model.predict(x_val_smote)
# confusion_matrix(y_val_smote, rf_pred_smote)
# accuracy_score(y_val_smote, rf_pred_smote)
# roc_auc_score(y_val_smote, rf_pred_smote)


set_seed(14042025)
brf_model = brfc(
  criterion = 'log_loss',
  n_estimators = 1000,
  n_jobs = 10,
  min_samples_split = 2,
  max_depth = 10,
  random_state = 14042025, 
  verbose = 2,
  class_weight = class_wt)

brf_model.fit(x_train, y_train_bi)
brf_pred = brf_model.predict(x_val)
confusion_matrix(y_val_bi, brf_pred)
accuracy_score(y_val_bi, brf_pred)
roc_auc_score(y_val_bi, brf_pred)

# brf_model.fit(x_train_smote, y_train_smote)
# brf_pred = brf_model.predict(x_val_smote)
# confusion_matrix(y_val_smote, rf_pred_smote)
# accuracy_score(y_val_smote, rf_pred_smote)
# roc_auc_score(y_val_smote, rf_pred_smote)


# Gradient Boost
set_seed(14042025)
boost_model = gbc(
  loss = 'log_loss',
  n_estimators = 1000,
  #max_depth = 10,
  random_state = 14042025, 
  verbose = 2,
  learning_rate = .001)

with parallel_backend('threading', n_jobs = 10):
  boost_model.fit(x_train, y_train_bi)

boost_pred = boost_model.predict(x_val)
confusion_matrix(y_val_bi, boost_pred)
accuracy_score(y_val_bi, boost_pred)
roc_auc_score(y_val_bi, boost_pred)


# set_seed(14042025)
# rus_model = rus(
#   estimator = boost_model,
#   n_estimators = 1000,
#   random_state = 14042025, 
#   learning_rate = .001)
# 
# with parallel_backend('threading', n_jobs = 10):
#   rus_model.fit(x_train, y_train_bi)
# 
# rus_pred = rus_model.predict(x_val)
# 
# confusion_matrix(y_val_bi, rus_pred)
# accuracy_score(y_val_bi, rus_pred)
# roc_auc_score(y_val_bi, rus_pred)

```


```{python}
#sep_q123 = (
#  model_laps
#  .loc[(model_laps['session'] == 'q'),
#  ['Driver', 'year', 'time_sec_x', 'lap_time_sec']]
#  .sort_values(['year', 'time_sec_x'], ascending = True)
#)
# 
# sep_q123.head()
# 
# sep_q123['time_min'] = sep_q123['time_sec_x']/60
# sep_q123['lap_min'] = sep_q123['lap_time_sec']/60
# 
# # final time needs to be calculated for each driver
# # then use the following calculations
# 
# q3_drivers_final = (
#   sep_q123
#   .loc[(sep_q123['Driver']
#   .isin(['VER', 'ALO', 'SAI', 'HAM', 'MAG', 'SCH', 'OCO', 'RUS', 'RIC', 'ZHO']))]
#   .groupby(['year', 'Driver'])
#   .tail(1)
# )
# 
# q3_drivers_final.head()
# 
# q2_drivers_final = (
#   sep_q123
#   .loc[(sep_q123['Driver']
#   .isin(['VER', 'ALO', 'SAI', 'HAM', 'MAG', 'SCH', 'OCO', 'RUS', 'RIC', 'ZHO',
#   'BOT', 'ALB', 'PER', 'NOR']))]
#   .groupby(['year', 'Driver'])
#   .tail(1)
# )
# 
# q2_drivers_final.head()
# 
# q1_drivers_final = (
#   sep_q123
#   .groupby(['year', 'Driver'])
#   .tail(1)
# )
# 
# q1_drivers_final.head()
# 
# q3_drivers_final['q3_start'] = q3_drivers_final['time_min'] - 12
# 
# q3_drivers_final.sort_values('time_min', ascending = True)
# 
# driver_final_time['q3'] = driver_final_time['time_min'] - 12
# driver_final_time['q2_to_q3'] = driver_final_time['q3'] - 8
# driver_final_time['q2'] = driver_final_time['q2_to_q3'] - 15
# driver_final_time['q1_to_q2'] = driver_final_time['q2'] - 7
# driver_final_time['q1'] = driver_final_time['q1_to_q2'] - 18
# 
# driver_final_time.head()
# 
# #final_time = sep_q123['time_min'].iloc[-1]
# 
# #q3 = 12 min
# #break between q2 and q3 = 8 min
# #q2 = 15 min
# # break between q1 and q2 = 7 min
# #q1 = 18 min
# 
# q3 = final_time - 12
# q2_to_q3 = q3 - 8
# q2 = q2_to_q3 - 15
# q1_to_q2 = q2 - 7
# q1 = q1_to_q2 - 18
# 
# def assign_sect(ts):
#     if ts > q3:
#         return 'q3'
#     elif ts > q2_to_q3:
#         return 'q2_to_q3'
#     elif ts > q2:
#         return 'q2'
#     elif ts > q1_to_q2:
#         return 'q1_to_q2'
#     elif ts > q1:
#         return 'q1'
#     else:
#         return 'startup'
# 
# sep_q123['q_sect'] = sep_q123['time_min'].apply(assign_sect)
# 
# sep_q123.head()
# 
# (
# 	sep_q123
# 	.loc[~sep_q123['q_sect'].isin(['q1_to_q2', 'q2_to_q3', 'startup'])]
# 	.groupby(['Driver', 'q_sect'])
# 	.min()
# 	.reset_index()
# )
```

# Modeling

```{python}
#| eval: false
#| echo: false

brf_model_smote_grid = brfc(
  criterion = 'log_loss',
  n_jobs = 10,
  random_state = 14042025, 
  n_estimators = 1000,
  verbose = 2)

set_seed(14042025)
brf_grid = (
  GridSearchCV(brf_model_smote_grid,
  rf_params,
  n_jobs = 10,
  cv = 3,
  verbose = 2)
)

brf_grid.fit(x_train_smote, y_train_smote)

# print(brf_grid.cv_results_)
print(brf_grid.best_score_)
# print(brf_grid.best_params_)
print(brf_grid.best_estimator_)
```

```{python}
boost_params = {'max_depth': [10, 14, 18, 20], 'min_samples_leaf': [1, 3, 6, 10], 'learning_rate': [.01, .001]}

boost_model_smote_grid = gbc(
  loss = 'log_loss',
  n_estimators = 1000,
  random_state = 14042025, 
  verbose = 2)

set_seed(14042025)
boost_grid = (
  GridSearchCV(boost_model_smote_grid,
  boost_params,
  cv = 5,
  verbose = 2)
)

with parallel_backend('threading', n_jobs = 10):
  boost_grid.fit(x_train_smote, y_train_smote)

#print(boost_grid.cv_results_)
print(boost_grid.best_score_)
print(boost_grid.best_params_)
print(boost_grid.best_estimator_)
```


```{python}
set_seed(14042025)
rf_model_smote = rfc(
  criterion = 'log_loss',
  n_estimators = 1000,
  n_jobs = 10,
  min_samples_split = 2,
  min_samples_leaf = 1,
  max_depth = 14,
  random_state = 14042025, 
  verbose = 2)

rf_model_smote.fit(x_train_smote, y_train_smote)
rf_pred_smote = rf_model_smote.predict(x_val_smote)
confusion_matrix(y_val_smote, rf_pred_smote)
accuracy_score(y_val_smote, rf_pred_smote)
roc_auc_score(y_val_smote, rf_pred_smote)
```

# Boosted Model

```{python}
set_seed(14042025)
boost_model = gbc(
  loss = 'log_loss',
  n_estimators = 1000,
  #max_depth = 10,
  random_state = 14042025, 
  verbose = 2,
  learning_rate = .001)

with parallel_backend('threading', n_jobs = 10):
  boost_model.fit(x_train, y_train_bi)

boost_pred = boost_model.predict(x_val)
confusion_matrix(y_val_bi, boost_pred)
accuracy_score(y_val_bi, boost_pred)
balanced_accuracy_score(y_val_bi, boost_pred)
roc_auc_score(y_val_bi, boost_pred)
```

```{python}
set_seed(14042025)
boost_model_smote = gbc(
  loss = 'log_loss',
  n_estimators = 1000,
  #max_depth = 10,
  random_state = 14042025, 
  verbose = 2,
  learning_rate = .001)

with parallel_backend('threading', n_jobs = 10):
  boost_model_smote.fit(x_train_smote, y_train_smote)

boost_pred_smote = boost_model_smote.predict(x_val_smote)
confusion_matrix(y_val_smote, boost_pred_smote)
accuracy_score(y_val_smote, boost_pred_smote)
balanced_accuracy_score(y_val_smote, boost_pred_smote)
roc_auc_score(y_val_smote, boost_pred_smote)
```


# Encoding Predictors


```{python}

model_laps.info()
model_laps.head()

model_laps[pd.get_dummies(model_laps['Driver'], dtype = 'float64').columns] = pd.get_dummies(model_laps['Driver'], dtype = 'float64')
# removed driver number because it correlates completely to the Driver column
# model_laps[pd.get_dummies(model_laps['DriverNumber'], dtype = 'float64').columns] = pd.get_dummies(model_laps['DriverNumber'], dtype = 'float64')
model_laps[['fp1', 'fp2', 'fp3', 'q', 'r']] = pd.get_dummies(model_laps['session'], dtype = 'float64')
model_laps[['hard', 'intermediate', 'medium', 'soft', 'wet']] = pd.get_dummies(model_laps['Compound'], dtype = 'float64')
model_laps['fresh_tires'] = pd.get_dummies(model_laps['FreshTyre'], dtype = 'float64').iloc[:, 1]
model_laps['rain'] = pd.get_dummies(model_laps['Rainfall'], dtype = 'float64').iloc[:, 1]
model_laps[['q1', 'q2', 'q3']] = pd.get_dummies(model_laps['final_q'], dtype = 'float64')

model_laps.head()
model_laps.columns.tolist()

model_laps = (
  model_laps
  .drop(columns = ['Driver', 'DriverNumber', 'session', 'Compound', 'FreshTyre', 'Rainfall', 'time_join',
  'time_sec_x', 'pit_out_time_sec', 'pit_in_time_sec_shift', 'lap_time_sec', 'final_q'])
  #'VER', 'q', 'medium', 'q1'])
)

model_laps.shape
model_laps.dropna().shape

model_laps = model_laps.dropna()
model_laps['position_bi'] = np.where(model_laps['Position'] == 1, 1, 0)
```

# Spliting Data Into Training, Validation, & Testing Data

```{python}
# model_laps_train = model_laps.loc[((model_laps['year'].isin([2022, 2023])) & (model_laps['r'] == 1)) | ((model_laps['year'] != 2024) & (model_laps['r'] != 1))]
# model_laps_test = model_laps.loc[((model_laps['year'] == 2024) & (model_laps['r'] == 1))]

model_laps_train = model_laps.loc[(model_laps['year'].isin([2022, 2023]))]
model_laps_test = model_laps.loc[((model_laps['year'] == 2024))]

x_train = model_laps_train.drop(columns = ['Position', 'position_bi']).loc[(model_laps_train['year'] == 2022)]
x_val = model_laps_train.drop(columns = ['Position', 'position_bi']).loc[(model_laps_train['year'] == 2023)]
x_test = model_laps_test.drop(columns = ['Position', 'position_bi'])

y_train_bi = model_laps_train.loc[(model_laps_train['year'] == 2022), 'position_bi']
y_val_bi =  model_laps_train.loc[(model_laps_train['year'] == 2023), 'position_bi']
y_test_bi = model_laps_test['position_bi']
```

# SMOTE (Optional)

```{python}
from imblearn.over_sampling import SMOTE

sm = SMOTE(sampling_strategy = 1, random_state = 14042025)
x_train_smote, y_train_smote = sm.fit_resample(x_train, y_train_bi)
x_val_smote, y_val_smote = sm.fit_resample(x_val, y_val_bi)
x_test_smote, y_test_smote = sm.fit_resample(x_test, y_test_bi)

from collections import Counter
Counter(y_train_bi)
Counter(y_train_smote)

from sklearn.utils.class_weight import compute_class_weight

# class_wt = compute_class_weight('balanced', classes = np.unique(y_train_bi), y = y_train_bi)
# class_wt = dict(enumerate(class_wt))

# class_wt_smote = compute_class_weight('balanced', classes = np.unique(y_train_smote), y = y_train_smote)
# class_wt_smote = dict(enumerate(class_wt))

from tensorflow.random import set_seed
from sklearn.model_selection import TimeSeriesSplit, KFold, GridSearchCV
from sklearn.metrics import accuracy_score, roc_auc_score, mean_absolute_error, roc_curve, precision_score, recall_score, make_scorer, log_loss, confusion_matrix, balanced_accuracy_score
from sklearn.ensemble import RandomForestClassifier as rfc, GradientBoostingClassifier as gbc
from imblearn.ensemble import BalancedRandomForestClassifier as brfc, RUSBoostClassifier as rus
from joblib import parallel_backend, Parallel, parallel_config
```

# Neural Network

```{python}
import tensorflow as tf
from tensorflow import keras
import keras_tuner as kt
import tensorflow.keras.models as keras_model
from tensorflow.keras.regularizers import l1_l2
from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, BatchNormalization, LayerNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.metrics import SparseCategoricalAccuracy, BinaryAccuracy, TruePositives, TrueNegatives, FalsePositives, FalseNegatives, AUC, Precision, Recall
from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy, BinaryFocalCrossentropy

num_classes = np.unique(y_train_bi)

class_wt = compute_class_weight('balanced', classes = num_classes, y = y_train_bi)
class_wt = dict(enumerate(class_wt))

x_train_nn = np.array(x_train)
x_val_nn = np.array(x_val)
x_test_nn = np.array(x_test)

y_train_nn = np.array(y_train_bi)
y_val_nn = np.array(y_val_bi)
y_test_nn = np.array(y_test_bi)

# SMOTE Upsampling
x_train_nn_sm = np.array(x_train_smote)
x_val_nn_sm = np.array(x_val_smote)
x_test_nn_sm = np.array(x_test_smote)

y_train_nn_sm = np.array(y_train_smote)
y_val_nn_sm = np.array(y_val_smote)
y_test_nn_sm = np.array(y_test_smote)

metric_bi = [
  BinaryAccuracy(name = 'accuracy'),
  AUC(name = 'roc_auc'),
  Precision(name = 'precision'),
  Recall(name = 'recall')
]

loss_bi = [
  BinaryCrossentropy(name = 'cross_entropy'),
  BinaryFocalCrossentropy(name = 'focal')
]
```

```{python}
set_seed(14042025)
def build_model(hp):
    model = keras_model.Sequential()

    hp_unit1 = hp.Choice("units", values = [32, 64, 128, 256, 512])
    hp_l1_1 = hp.Float('l1_pen1', min_value = 0, max_value = 1, step = .1, default = 0)
    hp_l2_1 = hp.Float('l2_pen1', min_value = 0, max_value = 1, step = .1, default = 0)
    model.add(Dense(hp_unit1, activation = 'relu', kernel_regularizer = l1_l2(l1 = hp_l1_1, l2 = hp_l2_1), input_shape = (x_train.shape[1],)))
    model.add(LayerNormalization())
    hp_dropout1 = hp.Float('dropout1', min_value = .1, max_value = .7, step = .05, default = .1)
    model.add(Dropout(hp_dropout1))

    hp_unit2 = hp.Choice("units2", values = [32, 64, 128, 256, 512])
    hp_l1_2 = hp.Float('l1_pen2', min_value = 0, max_value = 1, step = .1, default = 0)
    hp_l2_2 = hp.Float('l2_pen2', min_value = 0, max_value = 1, step = .1, default = 0)
    model.add(Dense(hp_unit2, activation = 'relu', kernel_regularizer = l1_l2(l1 = hp_l1_2, l2 = hp_l2_2)))
    model.add(LayerNormalization())
    hp_dropout2 = hp.Float('dropout2', min_value = .1, max_value = .7, step = .05, default = .1)
    model.add(Dropout(hp_dropout2))

    hp_unit3 = hp.Choice("units3", values = [8, 16, 32, 64, 128, 256, 512])
    hp_l1_3 = hp.Float('l1_pen3', min_value = 0, max_value = 1, step = .1, default = 0)
    hp_l2_3 = hp.Float('l2_pen3', min_value = 0, max_value = 1, step = .1, default = 0)
    model.add(Dense(hp_unit3, activation = 'relu', kernel_regularizer = l1_l2(l1 = hp_l1_3, l2 = hp_l2_3)))
    model.add(LayerNormalization())
    hp_dropout3 = hp.Float('dropout3', min_value = .1, max_value = .7, step = .05, default = .1)
    model.add(Dropout(hp_dropout3))

    #model.add(Dense(20, activation = 'softmax'))
    model.add(Dense(1, activation = 'sigmoid'))

    learn_rate = hp.Float("learning_rate", min_value = .0001, max_value = .01, step = .01)

    model.compile(
      optimizer = tf.keras.optimizers.AdamW(learning_rate = learn_rate),
      loss = loss_bi,
      metrics = metric_bi)

    return model
```

```{python}
set_seed(14042025)
tuner = kt.RandomSearch(
  build_model,
  objective = 'binary_accuracy',
  max_trials = 10,
  executions_per_trial = 1,
  project_name = 'tuned_gp'
)

set_seed(14042025)
tuner.search(x_train_nn_sm,
             y_train_nn_sm,
             #validation_split=0.2,
             epochs = 50,
             batch_size = 2048,
             callbacks = [keras.callbacks.EarlyStopping(patience = 5)])

best_model = tuner.get_best_models(num_models = 1)[0]
best_hps = tuner.get_best_hyperparameters(1)[0]

set_seed(14042025)
best_model.evaluate(x_val_nn_sm, y_val_nn_sm)
# ROC AUC = .49 w/o SMOTE
# Binary Accuracy = .93 w/o SMOTE

# ROC AUC = .33 w/ SMOTE
# Binary Accuracy = .66 w/ SMOTE

best_hps.values
```

```{python}
y_pred_nn = best_model.predict(x_val_nn_sm)
y_pred_nn_class = np.where(y_pred_nn > .5, 1, 0)

confusion_matrix(y_val_nn_sm, y_pred_nn_class)
accuracy_score(y_val_nn_sm, y_pred_nn_class)
roc_auc_score(y_val_nn_sm, y_pred_nn_class)
```

```{python}
def build_one_model():
    model = keras_model.Sequential()

    model.add(Dense(128, activation = 'relu', input_shape = (x_train.shape[1],)))
    model.add(LayerNormalization())
    model.add(Dropout(.4))

    model.add(Dense(128, activation = 'relu'))
    model.add(LayerNormalization())
    model.add(Dropout(.4))

    model.add(Dense(128, activation = 'relu'))
    model.add(LayerNormalization())
    model.add(Dropout(.4))

    #model.add(Dense(20, activation = 'softmax'))
    model.add(Dense(1, activation = 'sigmoid'))

    model.compile(
      optimizer = tf.keras.optimizers.AdamW(learning_rate = .0001),
      loss = loss_bi,
      metrics = metric_bi)

    return model
```

```{python}
nn = build_one_model()

set_seed(14042025)
(
  nn.fit(
    x_train_nn,
    y_train_nn,
    epochs = 50,
    batch_size = 256,
    verbose = 2,
    class_weight = {0: .3, 1: 25}
    #class_weight = class_wt
    #validation_data = (x_val_nn, y_val_nn)
    )
)

nn.evaluate(x_val_nn, y_val_nn, verbose = 2)
#loss, accuracy, roc_auc, precision, recall, cross_entropy_loss, focal_loss
```

```{python}
y_pred1 = nn.predict(x_val_nn)
y_pred1_class = np.where(y_pred1 > .5, 1, 0)

# pn.ggplot.show(
#   pn.ggplot(pn.aes(x = pd.DataFrame(y_pred1)[0]))
#   + pn.geom_histogram(bins = 10)
# )

print(confusion_matrix(y_val_nn, y_pred1_class))

print(accuracy_score(y_val_nn, y_pred1_class))
print(roc_auc_score(y_val_nn, y_pred1_class))
print(precision_score(y_val_nn, y_pred1_class))
print(recall_score(y_val_nn, y_pred1_class))
```

# K-Fold 

```{python}
#| eval: false
#| echo: false

rf_params = {'max_depth': [10, 14, 18, 20], 'min_samples_split': [2, 3, 4, 6], 'min_samples_leaf': [1, 3, 6, 10]}

rf_model_smote_grid = rfc(
  criterion = 'log_loss',
  n_jobs = 10,
  random_state = 14042025, 
  n_estimators = 1000,
  verbose = 2)

set_seed(14042025)
rf_grid = (
  GridSearchCV(rf_model_smote_grid,
  rf_params,
  cv = 5,
  verbose = 2)
)

rf_grid.fit(x_train_smote, y_train_smote)

# print(rf_grid.cv_results_)
print(rf_grid.best_score_)
print(rf_grid.best_params_)
print(rf_grid.best_estimator_)
```


# Balanced Random Forest Classifier

```{python}
set_seed(14042025)
brf_model = brfc(
  criterion = 'log_loss',
  n_estimators = 1000,
  n_jobs = 10,
  min_samples_split = 2,
  min_samples_leaf = 1,
  max_depth = 14,
  random_state = 14042025, 
  verbose = 2,
  class_weight = 'balanced')

brf_model.fit(x_train, y_train_bi)
brf_pred = brf_model.predict(x_val)
confusion_matrix(y_val_bi, brf_pred)
# accuracy_score(y_val_bi, brf_pred)
balanced_accuracy_score(y_val_bi, brf_pred)
roc_auc_score(y_val_bi, brf_pred)
```

```{python}
#| eval: false
#| echo: false

set_seed(14042025)
brf_model_smote = brfc(
  criterion = 'log_loss',
  n_estimators = 1000,
  n_jobs = 10,
  min_samples_split = 2,
  max_depth = 14,
  min_samples_leaf = 1,
  random_state = 14042025, 
  verbose = 2)

brf_model_smote.fit(x_train_smote, y_train_smote)
brf_pred_smote = brf_model_smote.predict(x_val_smote)
confusion_matrix(y_val_smote, brf_pred_smote)
#accuracy_score(y_val_bi, brf_pred)
balanced_accuracy_score(y_val_smote, brf_pred_smote)
roc_auc_score(y_val_smote, brf_pred_smote)
```

# Feature Importance

```{python}
from sklearn.inspection import permutation_importance as perm_import

# random forest
brf_perm_import_find = perm_import(brf_model, x_train, y_train_bi, n_repeats = 10, n_jobs = 10, random_state = 14042025) 

brf_import = (
  pd
  .DataFrame({'var_names': x_train.columns,
  'importance_mean': brf_perm_import_find.importances_mean})
  .sort_values('importance_mean', ascending = False)
)

pn.ggplot.show(
  pn.ggplot(brf_import, pn.aes('var_names', 'importance_mean'))
  + pn.geom_col(fill = 'dodgerblue')
  + pn.coord_flip()
  + pn.theme_light()
)

# balanced random forest
brf_perm_import_find_smote = perm_import(brf_model_smote, x_train_smote, y_train_smote, n_repeats = 10, n_jobs = 10, random_state = 14042025) 

brf_import_smote = (
  pd
  .DataFrame({'var_names': x_train.columns,
  'importance_mean': brf_perm_import_find_smote.importances_mean})
  .sort_values('importance_mean', ascending = False)
)

pn.ggplot.show(
  pn.ggplot(brf_import_smote, pn.aes('var_names', 'importance_mean'))
  + pn.geom_col(fill = 'dodgerblue')
  + pn.coord_flip()
  + pn.theme_light()
)
```

# Predictions on Testing Data

```{python}
#| eval: false
#| echo: false

brf_pred_test = brf_model.predict(x_test)
confusion_matrix(y_test_bi, brf_pred_test)
accuracy_score(y_test_bi, brf_pred_test)
balanced_accuracy_score(y_test_bi, brf_pred_test)
roc_auc_score(y_test_bi, brf_pred_test)

brf_pred_test_smote = brf_model_smote.predict(x_test_smote)
confusion_matrix(y_test_smote, brf_pred_test_smote)
accuracy_score(y_test_smote, brf_pred_test_smote)
balanced_accuracy_score(y_test_smote, brf_pred_test_smote)
roc_auc_score(y_test_smote, brf_pred_test_smote)
```

# Predictions on All Data

```{python}

```

# Predictions on New Data

```{python}
predictions = model.predict_proba(new_race)
winner_index = predictions.argmax()
likely_winner = new_race.iloc[winner_index]['racer']

print(f"Most likely winner in the rain: {likely_winner}")

```